{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import operator\n",
    "import warnings\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "doc_complete = []\n",
    "with open('petitions_complete.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        doc_complete.append(row) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_unicode(text):\n",
    "    if type(text) == str:\n",
    "    # Ignore errors even if the string is not proper UTF-8 or has\n",
    "    # broken marker bytes.\n",
    "    # Python built-in function unicode() can do this.\n",
    "        return unicode(text, \"utf-8\", errors=\"ignore\")\n",
    "    else:\n",
    "        # Assume the value object has proper __unicode__() method\n",
    "        return unicode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from gensim.utils import lemmatize\n",
    "import string\n",
    "\n",
    "# clean 1 without train_texts\n",
    "stops = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "ignore = [] # [\"http_www\",\"com\",\"html\",\"year_old\"]\n",
    "# manualy created ignore word list\n",
    "def clean1(texts):\n",
    "    texts = [[word for word in line if word not in stops] for line in texts]\n",
    "    texts = [[word.split('/')[0] for word in lemmatize(' '.join(line), allowed_tags=re.compile('(NN)'), min_length=3)] for line in texts]\n",
    "    texts = [[word for word in line if word not in exclude] for line in texts]\n",
    "    texts = [[word for word in line if word not in ignore] for line in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.6 s, sys: 244 ms, total: 39.9 s\n",
      "Wall time: 39.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_texts = []\n",
    "for doc in doc_complete:\n",
    "    text = to_unicode(doc['title']) + \" \" + to_unicode(doc['body'])\n",
    "    text = gensim.utils.simple_preprocess(text, deacc=True, min_len=3)\n",
    "    train_texts.append(text)\n",
    "    \n",
    "train_texts = clean1(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-06 15:46:43,139 : INFO : collecting all words and their counts\n",
      "2017-12-06 15:46:43,140 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2017-12-06 15:46:43,801 : INFO : collected 111051 word types from a corpus of 148088 words (unigram + bigrams) and 4095 sentences\n",
      "2017-12-06 15:46:43,802 : INFO : using 111051 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/phrases.py:316: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "bigram = gensim.models.Phrases(train_texts)\n",
    "train_texts = [bigram[line] for line in train_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top n (=20) common words that are going to be ignored.\n",
    "from collections import Counter\n",
    "flat_texts = [j for i in train_texts for j in i]\n",
    "top_words = Counter(flat_texts).most_common(1000)\n",
    "common_words = []\n",
    "for word, frequency in top_words:\n",
    "    if (len(common_words) < 20):\n",
    "        common_words.append(word)\n",
    "\n",
    "def clean2(texts):\n",
    "    texts = [[word for word in line if word not in common_words] for line in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = clean2(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4095"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Word Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-06 15:47:01,556 : DEBUG : Fast version of gensim.models.word2vec is being used\n",
      "2017-12-06 15:47:01,558 : INFO : collecting all words and their counts\n",
      "2017-12-06 15:47:01,559 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-06 15:47:01,602 : INFO : collected 11250 word types from a corpus of 119952 raw words and 4095 sentences\n",
      "2017-12-06 15:47:01,603 : INFO : Loading a fresh vocabulary\n",
      "2017-12-06 15:47:01,643 : INFO : min_count=1 retains 11250 unique words (100% of original 11250, drops 0)\n",
      "2017-12-06 15:47:01,644 : INFO : min_count=1 leaves 119952 word corpus (100% of original 119952, drops 0)\n",
      "2017-12-06 15:47:01,703 : INFO : deleting the raw counts dictionary of 11250 items\n",
      "2017-12-06 15:47:01,704 : INFO : sample=0.001 downsamples 13 most-common words\n",
      "2017-12-06 15:47:01,705 : INFO : downsampling leaves estimated 119388 word corpus (99.5% of prior 119952)\n",
      "2017-12-06 15:47:01,706 : INFO : estimated required memory for 11250 words and 1000 dimensions: 95625000 bytes\n",
      "2017-12-06 15:47:01,758 : INFO : resetting layer weights\n",
      "2017-12-06 15:47:02,184 : INFO : training model with 4 workers on 11250 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-06 15:47:02,187 : DEBUG : queueing job #0 (9994 words, 310 sentences) at alpha 0.02500\n",
      "2017-12-06 15:47:02,189 : DEBUG : queueing job #1 (9972 words, 375 sentences) at alpha 0.02462\n",
      "2017-12-06 15:47:02,196 : DEBUG : queueing job #2 (9993 words, 348 sentences) at alpha 0.02417\n",
      "2017-12-06 15:47:02,205 : DEBUG : queueing job #3 (9999 words, 330 sentences) at alpha 0.02374\n",
      "2017-12-06 15:47:02,228 : DEBUG : queueing job #4 (9955 words, 330 sentences) at alpha 0.02334\n",
      "2017-12-06 15:47:02,231 : DEBUG : queueing job #5 (9965 words, 346 sentences) at alpha 0.02294\n",
      "2017-12-06 15:47:02,234 : DEBUG : queueing job #6 (9990 words, 330 sentences) at alpha 0.02252\n",
      "2017-12-06 15:47:02,237 : DEBUG : queueing job #7 (9977 words, 337 sentences) at alpha 0.02212\n",
      "2017-12-06 15:47:02,240 : DEBUG : queueing job #8 (9976 words, 332 sentences) at alpha 0.02171\n",
      "2017-12-06 15:47:02,243 : DEBUG : queueing job #9 (9972 words, 354 sentences) at alpha 0.02131\n",
      "2017-12-06 15:47:02,249 : DEBUG : queueing job #10 (9990 words, 339 sentences) at alpha 0.02087\n",
      "2017-12-06 15:47:02,254 : DEBUG : queueing job #11 (9995 words, 359 sentences) at alpha 0.02046\n",
      "2017-12-06 15:47:02,266 : DEBUG : queueing job #12 (9998 words, 309 sentences) at alpha 0.02003\n",
      "2017-12-06 15:47:02,372 : DEBUG : queueing job #13 (9990 words, 374 sentences) at alpha 0.01965\n",
      "2017-12-06 15:47:02,383 : DEBUG : queueing job #14 (9993 words, 350 sentences) at alpha 0.01920\n",
      "2017-12-06 15:47:02,385 : DEBUG : queueing job #15 (9975 words, 329 sentences) at alpha 0.01877\n",
      "2017-12-06 15:47:02,388 : DEBUG : queueing job #16 (9989 words, 331 sentences) at alpha 0.01837\n",
      "2017-12-06 15:47:02,485 : DEBUG : queueing job #17 (9998 words, 347 sentences) at alpha 0.01797\n",
      "2017-12-06 15:47:02,500 : DEBUG : queueing job #18 (9997 words, 330 sentences) at alpha 0.01755\n",
      "2017-12-06 15:47:02,513 : DEBUG : queueing job #19 (9977 words, 337 sentences) at alpha 0.01714\n",
      "2017-12-06 15:47:02,526 : DEBUG : queueing job #20 (9966 words, 331 sentences) at alpha 0.01673\n",
      "2017-12-06 15:47:02,613 : DEBUG : queueing job #21 (9975 words, 355 sentences) at alpha 0.01633\n",
      "2017-12-06 15:47:02,640 : DEBUG : queueing job #22 (9971 words, 338 sentences) at alpha 0.01590\n",
      "2017-12-06 15:47:02,651 : DEBUG : queueing job #23 (9999 words, 360 sentences) at alpha 0.01549\n",
      "2017-12-06 15:47:02,677 : DEBUG : queueing job #24 (9984 words, 309 sentences) at alpha 0.01505\n",
      "2017-12-06 15:47:02,761 : DEBUG : queueing job #25 (9969 words, 373 sentences) at alpha 0.01468\n",
      "2017-12-06 15:47:02,811 : DEBUG : queueing job #26 (9969 words, 348 sentences) at alpha 0.01422\n",
      "2017-12-06 15:47:02,815 : DEBUG : queueing job #27 (9995 words, 331 sentences) at alpha 0.01380\n",
      "2017-12-06 15:47:02,833 : DEBUG : queueing job #28 (9977 words, 331 sentences) at alpha 0.01340\n",
      "2017-12-06 15:47:02,888 : DEBUG : queueing job #29 (9981 words, 347 sentences) at alpha 0.01299\n",
      "2017-12-06 15:47:02,937 : DEBUG : queueing job #30 (9994 words, 328 sentences) at alpha 0.01257\n",
      "2017-12-06 15:47:02,950 : DEBUG : queueing job #31 (9977 words, 338 sentences) at alpha 0.01217\n",
      "2017-12-06 15:47:02,980 : DEBUG : queueing job #32 (9967 words, 331 sentences) at alpha 0.01176\n",
      "2017-12-06 15:47:03,045 : DEBUG : queueing job #33 (9992 words, 356 sentences) at alpha 0.01136\n",
      "2017-12-06 15:47:03,072 : DEBUG : queueing job #34 (10000 words, 337 sentences) at alpha 0.01093\n",
      "2017-12-06 15:47:03,104 : DEBUG : queueing job #35 (9987 words, 359 sentences) at alpha 0.01052\n",
      "2017-12-06 15:47:03,136 : DEBUG : queueing job #36 (9998 words, 311 sentences) at alpha 0.01008\n",
      "2017-12-06 15:47:03,199 : INFO : PROGRESS: at 41.47% examples, 247659 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-06 15:47:03,201 : DEBUG : queueing job #37 (9992 words, 375 sentences) at alpha 0.00970\n",
      "2017-12-06 15:47:03,213 : DEBUG : queueing job #38 (9986 words, 348 sentences) at alpha 0.00925\n",
      "2017-12-06 15:47:03,246 : DEBUG : queueing job #39 (9998 words, 331 sentences) at alpha 0.00882\n",
      "2017-12-06 15:47:03,296 : DEBUG : queueing job #40 (9971 words, 331 sentences) at alpha 0.00842\n",
      "2017-12-06 15:47:03,343 : DEBUG : queueing job #41 (9999 words, 345 sentences) at alpha 0.00802\n",
      "2017-12-06 15:47:03,356 : DEBUG : queueing job #42 (9966 words, 329 sentences) at alpha 0.00760\n",
      "2017-12-06 15:47:03,397 : DEBUG : queueing job #43 (9999 words, 339 sentences) at alpha 0.00720\n",
      "2017-12-06 15:47:03,434 : DEBUG : queueing job #44 (9993 words, 332 sentences) at alpha 0.00678\n",
      "2017-12-06 15:47:03,493 : DEBUG : queueing job #45 (9983 words, 356 sentences) at alpha 0.00638\n",
      "2017-12-06 15:47:03,505 : DEBUG : queueing job #46 (9970 words, 335 sentences) at alpha 0.00595\n",
      "2017-12-06 15:47:03,544 : DEBUG : queueing job #47 (9989 words, 360 sentences) at alpha 0.00554\n",
      "2017-12-06 15:47:03,589 : DEBUG : queueing job #48 (9988 words, 311 sentences) at alpha 0.00510\n",
      "2017-12-06 15:47:03,648 : DEBUG : queueing job #49 (9994 words, 374 sentences) at alpha 0.00472\n",
      "2017-12-06 15:47:03,652 : DEBUG : queueing job #50 (9994 words, 348 sentences) at alpha 0.00427\n",
      "2017-12-06 15:47:03,693 : DEBUG : queueing job #51 (9968 words, 330 sentences) at alpha 0.00385\n",
      "2017-12-06 15:47:03,759 : DEBUG : queueing job #52 (9977 words, 331 sentences) at alpha 0.00345\n",
      "2017-12-06 15:47:03,780 : DEBUG : queueing job #53 (9987 words, 346 sentences) at alpha 0.00304\n",
      "2017-12-06 15:47:03,805 : DEBUG : queueing job #54 (9994 words, 330 sentences) at alpha 0.00262\n",
      "2017-12-06 15:47:03,843 : DEBUG : queueing job #55 (10000 words, 337 sentences) at alpha 0.00222\n",
      "2017-12-06 15:47:03,913 : DEBUG : queueing job #56 (9971 words, 333 sentences) at alpha 0.00181\n",
      "2017-12-06 15:47:03,929 : DEBUG : queueing job #57 (9976 words, 356 sentences) at alpha 0.00141\n",
      "2017-12-06 15:47:03,955 : DEBUG : queueing job #58 (9967 words, 333 sentences) at alpha 0.00097\n",
      "2017-12-06 15:47:03,997 : DEBUG : queueing job #59 (9999 words, 361 sentences) at alpha 0.00057\n",
      "2017-12-06 15:47:04,062 : DEBUG : queueing job #60 (703 words, 24 sentences) at alpha 0.00013\n",
      "2017-12-06 15:47:04,234 : DEBUG : job loop exiting, total 61 jobs\n",
      "2017-12-06 15:47:04,235 : INFO : PROGRESS: at 88.18% examples, 257311 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-06 15:47:04,390 : DEBUG : worker exiting, processed 15 jobs\n",
      "2017-12-06 15:47:04,390 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-06 15:47:04,394 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-06 15:47:04,394 : DEBUG : worker exiting, processed 16 jobs\n",
      "2017-12-06 15:47:04,421 : DEBUG : worker exiting, processed 15 jobs\n",
      "2017-12-06 15:47:04,422 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-06 15:47:04,455 : DEBUG : worker exiting, processed 15 jobs\n",
      "2017-12-06 15:47:04,455 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-06 15:47:04,457 : INFO : training on 599760 raw words (596952 effective words) took 2.3s, 263008 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "wv_model = Word2Vec(train_texts, size=1000, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99998262544756988"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.wv.similarity('gun', 'firearm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for CorEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4095, 11189)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import vis_topic as vt\n",
    "import corex_topic as ct\n",
    "import scipy.sparse as ss\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "raw_docs = [' '.join(clean_doc) for clean_doc in train_texts]\n",
    "\n",
    "# Transform cleaned petition texts into a sparse matrix\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features = 20000, binary = True)\n",
    "doc_word = vectorizer.fit_transform(raw_docs)\n",
    "doc_word = ss.csr_matrix(doc_word)\n",
    "\n",
    "doc_word.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4095, 11189)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all integers\n",
    "not_digit_inds = [ind for ind,word in enumerate(words) if not word.isdigit()]\n",
    "doc_word = doc_word[:,not_digit_inds]\n",
    "words = [word for ind,word in enumerate(words) if not word.isdigit()]\n",
    "\n",
    "doc_word.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: disease, patient, cancer, treatment, doctor, cure, disorder, medication, pain, awareness, symptom, illness\n",
      "1: man_power, consent_government, code_word, states_declaration, racist_racist, station_law, independence_course, nature_nature, racist_code, god_respect, power_earth, opinion_mankind\n",
      "2: god_trust, robbery_kidnapping, icon_max, rapper_conviction, year_september, wingate_rap, retrial_notice, christie_president, wingate_involvement, max_appeal, robbery_man, murder_feel\n",
      "3: ocean, nasa, april, bus, balance, dolphin, proclaim, explosion, cabinet, abuse_power, shame, hunt\n",
      "4: health, economy, tax, cost, benefit, increase, company, money, market, pay, healthcare, fund\n",
      "5: election, investigation, vote, voter, candidate, hillary_clinton, voting, campaign, department_justice, fbi, ballot, office\n",
      "6: internet, consumer, energy, information, technology, provider, service, device, car, access, fuel, safety\n",
      "7: democracy, war, freedom, russia, ukraine, party, liberty, sanction, respect, regime, principle, protester\n",
      "8: marijuana, animal, drug, plant, substance, gas, use, owner, cannabis, cruelty, pet, retrial\n",
      "9: student, school, education, college, teacher, university, sport, youth, muslim, contribution, graduate, community\n",
      "10: genocide, white, land, race, history, diversity, extinction, ecosystem, word, park, help_veteran, hunting\n",
      "11: racist, clinton, fan, http_goo, player, william, affair, nominee, winner, floor_vote, music, fighter\n",
      "12: gun, firearm, weapon, arm, amendment, shooting, polouse, dog, police_department, target, magazine, gun_violence\n",
      "13: visa, job, worker, income, immigration, program, work, immigrant, opportunity, degree, manufacturer, home\n",
      "14: terrorism, obama, terrorist, civilian, president_barack, isis, attack, massacre, event, terrorist_group, terror, group\n",
      "15: water, employment, competition, travel, investment, disaster, innovation, solution, application, waste, team, region\n",
      "16: officer, police, law_enforcement, evidence, police_officer, county, aircraft, judge, governor_chris, killing, conviction, department\n",
      "17: service_member, courage, head, protocol, video, white_house, installation, ground, defend, crew, camera, floor\n",
      "18: duty, soldier, defense, retirement, initiative, religion, flight, marine, army, personnel, speech, afghanistan\n",
      "19: security, death, organization, negotiation, support, half, combat, transgender, trial, intelligence, barack_obama, spouse\n",
      "CPU times: user 5.95 s, sys: 516 ms, total: 6.46 s\n",
      "Wall time: 6.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the CorEx topic model with 20 topics (Takes about 3.74s)\n",
    "seed = 1989\n",
    "num_topics = 20\n",
    "anchor_words = []\n",
    "\n",
    "ct_model = ct.Corex(n_hidden=num_topics, words=words, max_iter=20, verbose=False, seed=seed)\n",
    "ct_model.fit(doc_word, words=words);\n",
    "# Print all topics from the CorEx topic model\n",
    "topics = ct_model.get_topics()\n",
    "for n in range(num_topics):\n",
    "    topic_words,_ = zip(*ct_model.get_topics(topic=n, n_words=20))\n",
    "    print '{}: '.format(n) + ', '.join(topic_words[:12])\n",
    "    anchor_words.append(list(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate tsne cordniates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: `threshold` is deprecated!\n",
      "stats.threshold is deprecated in scipy 0.17.0\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "doc_vecs = []\n",
    "for i in range(len(doc_complete)):\n",
    "    doc_vec = list(ct_model.p_y_given_x[i])\n",
    "    doc_vecs.append(doc_vec)\n",
    "\n",
    "from scipy.stats import threshold\n",
    "thresholded_doc_vecs = threshold(doc_vecs, 0.1)\n",
    "\n",
    "topic_id = 4\n",
    "doc_ids = []\n",
    "for i in range(len(doc_complete)):\n",
    "    if (thresholded_doc_vecs[i][topic_id] > 0.5):\n",
    "        doc_ids.append(i)\n",
    "\n",
    "print(len(doc_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA is fast, but only capture linear relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.3 ms, sys: 4.43 ms, total: 21.8 ms\n",
      "Wall time: 17.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "    \n",
    "pca = PCA(n_components=2)\n",
    "fiftyDimVecs = pca.fit_transform([doc_vec for doc_vec in doc_vecs])\n",
    "# tsne = TSNE(n_components=2)\n",
    "# twoDimVecs = tsne.fit_transform(fiftyDimVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-25.02761078,  58.449543  ], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twoDimVecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE is too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 38s, sys: 6.29 s, total: 1min 44s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "X = np.array(thresholded_doc_vecs)\n",
    "X_embedded = TSNE(n_components=2, random_state=1989).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 43.06148911, -41.58058167], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embedded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write petition clustering to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "size = len(twoDimVecs)\n",
    "import json\n",
    "with open('tsne.json', 'a') as outfile:\n",
    "    lines = []\n",
    "    for i in range(size):\n",
    "        line = {'cord_x': str(twoDimVecs[i][0]),\n",
    "                'cord_y': str(twoDimVecs[i][1]),\n",
    "                'title': str(doc_complete[i]['title']),\n",
    "                'body': str(doc_complete[i]['body'])}\n",
    "        color = 0;\n",
    "        for j in range(num_topics):\n",
    "            line['topic_' + str(j)] = doc_vecs[i][j]\n",
    "            if (doc_vecs[i][j] > doc_vecs[i][color]):\n",
    "                color = j\n",
    "        line['topic_id'] = color\n",
    "        lines.append(line)\n",
    "    json.dump(lines, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Topic Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-06 15:54:55,033 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-12-06 15:54:55,259 : INFO : built Dictionary(11250 unique tokens: [u'sinjar', u'deferment', u'yellow', u'narcotic', u'vani']...) from 4095 documents (total 119952 corpus positions)\n",
      "2017-12-06 15:54:55,844 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2017-12-06 15:54:55,930 : INFO : 1 batches submitted to accumulate stats from 64 documents (1483 virtual)\n",
      "2017-12-06 15:54:55,936 : INFO : 2 batches submitted to accumulate stats from 128 documents (3073 virtual)\n",
      "2017-12-06 15:54:55,940 : INFO : 3 batches submitted to accumulate stats from 192 documents (4525 virtual)\n",
      "2017-12-06 15:54:55,945 : INFO : 4 batches submitted to accumulate stats from 256 documents (6204 virtual)\n",
      "2017-12-06 15:54:55,951 : INFO : 5 batches submitted to accumulate stats from 320 documents (7664 virtual)\n",
      "2017-12-06 15:54:55,958 : INFO : 6 batches submitted to accumulate stats from 384 documents (9211 virtual)\n",
      "2017-12-06 15:54:56,017 : INFO : 8 batches submitted to accumulate stats from 512 documents (10927 virtual)\n",
      "2017-12-06 15:54:56,023 : INFO : 9 batches submitted to accumulate stats from 576 documents (12407 virtual)\n",
      "2017-12-06 15:54:56,028 : INFO : 10 batches submitted to accumulate stats from 640 documents (13827 virtual)\n",
      "2017-12-06 15:54:56,032 : INFO : 11 batches submitted to accumulate stats from 704 documents (15048 virtual)\n",
      "2017-12-06 15:54:56,038 : INFO : 12 batches submitted to accumulate stats from 768 documents (16553 virtual)\n",
      "2017-12-06 15:54:56,013 : DEBUG : completed batch 0; 64 documents processed (1591 virtual)\n",
      "2017-12-06 15:54:56,043 : INFO : 13 batches submitted to accumulate stats from 832 documents (17967 virtual)\n",
      "2017-12-06 15:54:56,049 : INFO : 14 batches submitted to accumulate stats from 896 documents (19560 virtual)\n",
      "2017-12-06 15:54:56,006 : DEBUG : completed batch 0; 64 documents processed (1490 virtual)\n",
      "2017-12-06 15:54:56,034 : DEBUG : completed batch 0; 64 documents processed (646 virtual)\n",
      "2017-12-06 15:54:56,054 : INFO : 15 batches submitted to accumulate stats from 960 documents (21070 virtual)\n",
      "2017-12-06 15:54:56,058 : INFO : 16 batches submitted to accumulate stats from 1024 documents (22503 virtual)\n",
      "2017-12-06 15:54:56,064 : INFO : 17 batches submitted to accumulate stats from 1088 documents (24057 virtual)\n",
      "2017-12-06 15:54:56,056 : DEBUG : completed batch 0; 64 documents processed (1549 virtual)\n",
      "2017-12-06 15:54:56,047 : DEBUG : completed batch 0; 64 documents processed (1458 virtual)\n",
      "2017-12-06 15:54:56,069 : INFO : 18 batches submitted to accumulate stats from 1152 documents (25545 virtual)\n",
      "2017-12-06 15:54:56,075 : INFO : 19 batches submitted to accumulate stats from 1216 documents (27011 virtual)\n",
      "2017-12-06 15:54:56,081 : DEBUG : completed batch 0; 64 documents processed (1679 virtual)\n",
      "2017-12-06 15:54:56,090 : INFO : 20 batches submitted to accumulate stats from 1280 documents (28508 virtual)\n",
      "2017-12-06 15:54:56,067 : DEBUG : completed batch 0; 64 documents processed (1461 virtual)\n",
      "2017-12-06 15:54:56,106 : INFO : 21 batches submitted to accumulate stats from 1344 documents (29977 virtual)\n",
      "2017-12-06 15:54:56,123 : DEBUG : completed batch 1; 128 documents processed (2668 virtual)\n",
      "2017-12-06 15:54:56,137 : INFO : 22 batches submitted to accumulate stats from 1408 documents (31339 virtual)\n",
      "2017-12-06 15:54:56,134 : DEBUG : completed batch 1; 128 documents processed (2975 virtual)\n",
      "2017-12-06 15:54:56,152 : DEBUG : completed batch 1; 128 documents processed (2066 virtual)\n",
      "2017-12-06 15:54:56,151 : DEBUG : completed batch 1; 128 documents processed (2773 virtual)\n",
      "2017-12-06 15:54:56,156 : INFO : 23 batches submitted to accumulate stats from 1472 documents (32779 virtual)\n",
      "2017-12-06 15:54:56,160 : INFO : 24 batches submitted to accumulate stats from 1536 documents (34134 virtual)\n",
      "2017-12-06 15:54:56,165 : INFO : 25 batches submitted to accumulate stats from 1600 documents (35510 virtual)\n",
      "2017-12-06 15:54:56,164 : DEBUG : completed batch 1; 128 documents processed (2963 virtual)\n",
      "2017-12-06 15:54:56,179 : INFO : 26 batches submitted to accumulate stats from 1664 documents (37007 virtual)\n",
      "2017-12-06 15:54:56,175 : DEBUG : completed batch 1; 128 documents processed (3105 virtual)\n",
      "2017-12-06 15:54:56,190 : INFO : 27 batches submitted to accumulate stats from 1728 documents (38479 virtual)\n",
      "2017-12-06 15:54:56,185 : DEBUG : completed batch 1; 128 documents processed (3057 virtual)\n",
      "2017-12-06 15:54:56,199 : INFO : 28 batches submitted to accumulate stats from 1792 documents (40124 virtual)\n",
      "2017-12-06 15:54:56,222 : DEBUG : completed batch 2; 192 documents processed (4184 virtual)\n",
      "2017-12-06 15:54:56,234 : INFO : 29 batches submitted to accumulate stats from 1856 documents (41494 virtual)\n",
      "2017-12-06 15:54:56,233 : DEBUG : completed batch 2; 192 documents processed (4410 virtual)\n",
      "2017-12-06 15:54:56,245 : INFO : 30 batches submitted to accumulate stats from 1920 documents (42964 virtual)\n",
      "2017-12-06 15:54:56,264 : DEBUG : completed batch 2; 192 documents processed (3554 virtual)\n",
      "2017-12-06 15:54:56,267 : INFO : 31 batches submitted to accumulate stats from 1984 documents (44445 virtual)\n",
      "2017-12-06 15:54:56,264 : DEBUG : completed batch 2; 192 documents processed (4433 virtual)\n",
      "2017-12-06 15:54:56,272 : DEBUG : completed batch 2; 192 documents processed (4526 virtual)\n",
      "2017-12-06 15:54:56,266 : DEBUG : completed batch 2; 192 documents processed (4606 virtual)\n",
      "2017-12-06 15:54:56,275 : INFO : 32 batches submitted to accumulate stats from 2048 documents (45753 virtual)\n",
      "2017-12-06 15:54:56,272 : DEBUG : completed batch 2; 192 documents processed (4330 virtual)\n",
      "2017-12-06 15:54:56,280 : INFO : 33 batches submitted to accumulate stats from 2112 documents (47240 virtual)\n",
      "2017-12-06 15:54:56,284 : INFO : 34 batches submitted to accumulate stats from 2176 documents (48590 virtual)\n",
      "2017-12-06 15:54:56,289 : INFO : 35 batches submitted to accumulate stats from 2240 documents (49980 virtual)\n",
      "2017-12-06 15:54:56,319 : DEBUG : completed batch 3; 256 documents processed (5546 virtual)\n",
      "2017-12-06 15:54:56,322 : INFO : 36 batches submitted to accumulate stats from 2304 documents (51568 virtual)\n",
      "2017-12-06 15:54:56,330 : DEBUG : completed batch 3; 256 documents processed (5855 virtual)\n",
      "2017-12-06 15:54:56,345 : INFO : 37 batches submitted to accumulate stats from 2368 documents (52841 virtual)\n",
      "2017-12-06 15:54:56,355 : DEBUG : completed batch 3; 256 documents processed (4912 virtual)\n",
      "2017-12-06 15:54:56,358 : INFO : 38 batches submitted to accumulate stats from 2432 documents (54149 virtual)\n",
      "2017-12-06 15:54:56,355 : DEBUG : completed batch 3; 256 documents processed (5819 virtual)\n",
      "2017-12-06 15:54:56,365 : INFO : 39 batches submitted to accumulate stats from 2496 documents (55552 virtual)\n",
      "2017-12-06 15:54:56,371 : DEBUG : completed batch 3; 256 documents processed (5827 virtual)\n",
      "2017-12-06 15:54:56,374 : INFO : 40 batches submitted to accumulate stats from 2560 documents (57217 virtual)\n",
      "2017-12-06 15:54:56,373 : DEBUG : completed batch 3; 256 documents processed (6083 virtual)\n",
      "2017-12-06 15:54:56,378 : DEBUG : completed batch 3; 256 documents processed (6171 virtual)\n",
      "2017-12-06 15:54:56,386 : INFO : 41 batches submitted to accumulate stats from 2624 documents (58621 virtual)\n",
      "2017-12-06 15:54:56,391 : INFO : 42 batches submitted to accumulate stats from 2688 documents (60095 virtual)\n",
      "2017-12-06 15:54:56,409 : DEBUG : completed batch 4; 320 documents processed (6919 virtual)\n",
      "2017-12-06 15:54:56,413 : INFO : 43 batches submitted to accumulate stats from 2752 documents (61454 virtual)\n",
      "2017-12-06 15:54:56,435 : DEBUG : completed batch 4; 320 documents processed (7340 virtual)\n",
      "2017-12-06 15:54:56,446 : INFO : 44 batches submitted to accumulate stats from 2816 documents (62855 virtual)\n",
      "2017-12-06 15:54:56,442 : DEBUG : completed batch 4; 320 documents processed (7129 virtual)\n",
      "2017-12-06 15:54:56,449 : DEBUG : completed batch 4; 320 documents processed (6397 virtual)\n",
      "2017-12-06 15:54:56,453 : INFO : 45 batches submitted to accumulate stats from 2880 documents (64271 virtual)\n",
      "2017-12-06 15:54:56,457 : INFO : 46 batches submitted to accumulate stats from 2944 documents (65468 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-06 15:54:56,475 : DEBUG : completed batch 4; 320 documents processed (7475 virtual)\n",
      "2017-12-06 15:54:56,482 : INFO : 47 batches submitted to accumulate stats from 3008 documents (66862 virtual)\n",
      "2017-12-06 15:54:56,480 : DEBUG : completed batch 4; 320 documents processed (7316 virtual)\n",
      "2017-12-06 15:54:56,488 : INFO : 48 batches submitted to accumulate stats from 3072 documents (67350 virtual)\n",
      "2017-12-06 15:54:56,479 : DEBUG : completed batch 4; 320 documents processed (7528 virtual)\n",
      "2017-12-06 15:54:56,492 : DEBUG : completed batch 5; 384 documents processed (8507 virtual)\n",
      "2017-12-06 15:54:56,516 : DEBUG : completed batch 5; 384 documents processed (8445 virtual)\n",
      "2017-12-06 15:54:56,521 : DEBUG : completed batch 5; 384 documents processed (8619 virtual)\n",
      "2017-12-06 15:54:56,537 : DEBUG : completed batch 5; 384 documents processed (7801 virtual)\n",
      "2017-12-06 15:54:56,564 : DEBUG : completed batch 5; 384 documents processed (8722 virtual)\n",
      "2017-12-06 15:54:56,567 : DEBUG : completed batch 5; 384 documents processed (9140 virtual)\n",
      "2017-12-06 15:54:56,579 : DEBUG : completed batch 5; 384 documents processed (9006 virtual)\n",
      "2017-12-06 15:54:56,593 : DEBUG : completed batch 6; 448 documents processed (9872 virtual)\n",
      "2017-12-06 15:54:56,596 : DEBUG : observed sentinel value; terminating\n",
      "2017-12-06 15:54:56,598 : DEBUG : observed sentinel value; terminating\n",
      "2017-12-06 15:54:56,600 : DEBUG : finished all batches; 384 documents processed (9006 virtual)\n",
      "2017-12-06 15:54:56,604 : DEBUG : completed batch 6; 448 documents processed (10039 virtual)\n",
      "2017-12-06 15:54:56,609 : INFO : serializing accumulator to return to master...\n",
      "2017-12-06 15:54:56,613 : DEBUG : observed sentinel value; terminating\n",
      "2017-12-06 15:54:56,609 : DEBUG : finished all batches; 448 documents processed (9872 virtual)\n",
      "2017-12-06 15:54:56,618 : DEBUG : finished all batches; 448 documents processed (10039 virtual)\n",
      "2017-12-06 15:54:56,618 : DEBUG : completed batch 6; 448 documents processed (9003 virtual)\n",
      "2017-12-06 15:54:56,619 : DEBUG : completed batch 6; 405 documents processed (9628 virtual)\n",
      "2017-12-06 15:54:56,615 : DEBUG : completed batch 6; 448 documents processed (9848 virtual)\n",
      "2017-12-06 15:54:56,621 : INFO : serializing accumulator to return to master...\n",
      "2017-12-06 15:54:56,622 : DEBUG : observed sentinel value; terminating\n",
      "2017-12-06 15:54:56,622 : INFO : serializing accumulator to return to master...\n",
      "2017-12-06 15:54:56,623 : DEBUG : observed sentinel value; terminating\n",
      "2017-12-06 15:54:56,625 : INFO : accumulator serialized\n",
      "2017-12-06 15:54:56,625 : DEBUG : observed sentinel value; terminating\n",
      "2017-12-06 15:54:56,625 : DEBUG : finished all batches; 448 documents processed (9003 virtual)\n",
      "2017-12-06 15:54:56,627 : DEBUG : finished all batches; 405 documents processed (9628 virtual)\n",
      "2017-12-06 15:54:56,629 : INFO : accumulator serialized\n",
      "2017-12-06 15:54:56,629 : INFO : serializing accumulator to return to master...\n",
      "2017-12-06 15:54:56,629 : DEBUG : finished all batches; 448 documents processed (9848 virtual)\n",
      "2017-12-06 15:54:56,630 : INFO : serializing accumulator to return to master...\n",
      "2017-12-06 15:54:56,632 : INFO : serializing accumulator to return to master...\n",
      "2017-12-06 15:54:56,632 : INFO : accumulator serialized\n",
      "2017-12-06 15:54:56,632 : INFO : accumulator serialized\n",
      "2017-12-06 15:54:56,635 : INFO : accumulator serialized\n",
      "2017-12-06 15:54:56,618 : INFO : accumulator serialized\n",
      "2017-12-06 15:54:56,682 : INFO : accumulator serialized\n",
      "2017-12-06 15:54:56,670 : DEBUG : completed batch 6; 448 documents processed (10118 virtual)\n",
      "2017-12-06 15:54:56,672 : DEBUG : observed sentinel value; terminating\n",
      "2017-12-06 15:54:56,674 : DEBUG : finished all batches; 448 documents processed (10118 virtual)\n",
      "2017-12-06 15:54:56,676 : INFO : serializing accumulator to return to master...\n",
      "2017-12-06 15:54:56,886 : INFO : 7 accumulators retrieved from output queue\n",
      "2017-12-06 15:54:56,910 : INFO : accumulated word occurrence stats for 67514 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.470615583471\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(train_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in train_texts]\n",
    "\n",
    "cttopics = []\n",
    "for i in range(num_topics):\n",
    "    _words = ct_model.get_topics(topic=i)\n",
    "    cttopics.append((i, _words))\n",
    "_cttopics = [[word for word, prob in topic] for topicid, topic in cttopics]\n",
    "ct_coherence = CoherenceModel(topics=_cttopics[:10], texts=train_texts, dictionary=dictionary, window_size=10).get_coherence()\n",
    "print(ct_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[u'disease', u'patient', u'cancer', u'treatment', u'doctor', u'cure', u'disorder', u'medication', u'pain', u'awareness', u'symptom', u'illness', u'medicine', u'diagnosis', u'disability', u'brain', u'diabetes', u'condition', u'physician', u'organ']\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(anchor_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[u'health', u'economy', u'tax', u'cost', u'benefit', u'increase', u'company', u'money', u'market', u'pay', u'healthcare', u'fund', u'research', u'dollar', u'debt', u'industry', u'budget', u'study', u'business', u'product']\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(anchor_words[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'disease', u'patient', u'cancer', u'treatment', u'doctor', u'cure', u'disorder', u'medication', u'pain', u'awareness', u'symptom', u'illness', u'medicine', u'diagnosis', u'disability', u'brain', u'diabetes', u'condition', u'physician', u'organ', 'health']\n",
      "[u'economy', u'tax', u'cost', u'benefit', u'increase', u'company', u'money', u'market', u'pay', u'healthcare', u'fund', u'research', u'dollar', u'debt', u'industry', u'budget', u'study', u'business', u'product']\n"
     ]
    }
   ],
   "source": [
    "move_anchor_words = []\n",
    "for words in anchor_words:\n",
    "    move_anchor_words.append(list(words))\n",
    "move_anchor_words[0].append(\"health\")\n",
    "print str(move_anchor_words[0])\n",
    "move_anchor_words[4].remove(\"health\")\n",
    "print str(move_anchor_words[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: health, treatment, disease, condition, patient, doctor, cancer, awareness, pain, illness, medicine, disability\n",
      "1: man_power, consent_government, code_word, father_united, states_declaration, station_law, event_person, racist_racist, nature_nature, independence_course, band_assume, opinion_mankind\n",
      "2: god_trust, rapper_conviction, wingate_involvement, murder_feel, christie_president, wingate_rap, state_robbery, icon_max, year_september, max_appeal, retrial_notice, robbery_man\n",
      "3: april, hunt, ocean, nasa, explosion, cabinet, bus, balance, abuse_power, proclaim, dolphin, transit\n",
      "4: money, benefit, company, pay, economy, business, cost, fund, tax, industry, dollar, budget\n",
      "5: office, investigation, vote, election, campaign, candidate, corruption, voter, fraud, hillary_clinton, voting, attorney\n",
      "6: service, safety, information, com, access, standard, technology, internet, consumer, vehicle, energy, car\n",
      "7: power, war, freedom, force, party, democracy, leader, respect, ukraine, russia, liberty, sanction\n",
      "8: use, drug, marijuana, animal, sale, owner, plant, substance, cruelty, oil, chemical, gas\n",
      "9: school, community, education, student, science, college, youth, university, teacher, contribution, hate, graduate\n",
      "10: history, land, genocide, word, white, park, race, existence, diversity, hunting, extinction, ecosystem\n",
      "11: japan, racist, fan, player, music, http_goo, william, character, affair, floor_vote, winner, fighter\n",
      "12: protection, amendment, weapon, gun, firearm, property, polouse, arm, dog, shooting, tragedy, target\n",
      "13: help, home, job, work, program, worker, class, status, week, opportunity, visa, immigration\n",
      "14: group, event, attack, obama, terrorism, terrorist, civilian, military, isis, president_barack, iraq, massacre\n",
      "15: water, solution, region, development, travel, employment, team, application, disaster, investment, competition, infrastructure\n",
      "16: evidence, officer, department, police, law_enforcement, judge, killing, county, police_officer, justice_system, conviction, aircraft\n",
      "17: video, ground, title, head, service_member, facebook, defend, courage, installation, protocol, crew, floor\n",
      "18: duty, soldier, veteran, army, defense, personnel, church, danger, afghanistan, retirement, troop, kill\n",
      "19: support, death, policy, security, organization, trial, jail, difference, bank, half, negotiation, combat\n"
     ]
    }
   ],
   "source": [
    "ct_model_move = ct.Corex(n_hidden=(num_topics), words=words, max_iter=20, verbose=False, seed=seed)\n",
    "ct_model_move.fit(doc_word, words=words, anchors=move_anchor_words, anchor_strength=6);\n",
    "for n in range(num_topics):\n",
    "    topic_words,_ = zip(*ct_model_move.get_topics(topic=n, n_words=12))\n",
    "    print '{}: '.format(n) + ', '.join(topic_words[:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10855\n",
      "(4095, 10855)\n"
     ]
    }
   ],
   "source": [
    "## Remove a word in topic 3\n",
    "def remove_values_from_list(the_list, val):\n",
    "    return [value for value in the_list if value != val]\n",
    "\n",
    "words_to_remove = ['http', \"html\"]\n",
    "remove_train_texts = []\n",
    "for train_text in train_texts:\n",
    "    x = remove_values_from_list(train_text, \"\")\n",
    "    for word in words_to_remove:\n",
    "        x = remove_values_from_list(x, word)\n",
    "    remove_train_texts.append(x)\n",
    "        \n",
    "remove_raw_docs = [' '.join(clean_doc) for clean_doc in remove_train_texts]\n",
    "remove_vectorizer = CountVectorizer(stop_words='english', max_features = 20000, binary = True)\n",
    "remove_doc_word = remove_vectorizer.fit_transform(remove_raw_docs)\n",
    "remove_doc_word = ss.csr_matrix(remove_doc_word)\n",
    "remove_words = list(np.asarray(remove_vectorizer.get_feature_names()))\n",
    "\n",
    "print len(remove_words)\n",
    "print remove_doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_anchor_words = []\n",
    "for words in anchor_words:\n",
    "    x = remove_values_from_list(words, \"\")\n",
    "    for word in words_to_remove:\n",
    "        x = remove_values_from_list(x, word)\n",
    "    remove_anchor_words.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: country, word, plan, destruction, article, africa, diversity, everybody, african, assimilation, species, network, asian, conservation, http_goo, diversity_code, whole_part, jackson, trail, hunter\n",
      "1: release, band, separation, conviction, equal_station, course_human, independence, retrial, egypt, governor_chris, parliament, christie, max_appeal, icon_max, feel_rapper, range, michigan, railroad, moment, communist_party\n",
      "2: disease, awareness, cancer, pain, suffering, medicine, disability, disorder, cure, medication, treatment, physician, symptom, brain, diabetes, diagnosis, syndrome, aviation, flight, condition\n",
      "3: administration, party, animal, owner, sanction, regime, news, cruelty, human, opposition, chairman, consumption, channel, brutality, reporter, libya, fox, pet, cat, puppy\n",
      "4: prison, situation, defense, army, award, sentence, afghanistan, shot, zone, general, direction, compensation, vietnam, film, deployment, brother, afghan, sgt, deal, lincoln\n",
      "5: com, genocide, father, consent, racist, existence, white, racist_code, facebook, assume, convention, https_www, wife, goo, robbery, wingate, youtube, org, watch, video\n",
      "6: child, health, school, parent, family, patient, doctor, study, holiday, drug, adult, illness, baby, kid, quality, cost, day, practice, provider, state\n",
      "7: world, economy, area, budget, return, immigration, water, project, environment, taxe, investment, loan, infrastructure, asia, home, dollar, wage, construction, firearm, disaster\n",
      "8: right, law, constitution, amendment, violation, respect, government, opinion, liberty, earth, america, union, citizen, mankind, end, arm, founding, freedom, marriage, religion\n",
      "9: service, company, access, use, food, effect, regulation, product, resource, sale, income, plant, price, substance, revenue, employment, prohibition, relief, cannabis, murder\n",
      "10: information, industry, market, reform, consumer, technology, vehicle, energy, car, priority, employer, hillary_clinton, policy, clinton, production, fuel, sander, growth, bernie, traffic\n",
      "11: action, war, terrorist, attack, soldier, terrorism, peace, ukraine, medal, civilian, enemy, terror, protester, israel, massacre, petition, syrium, internet, syria, troop\n",
      "12: congress, job, benefit, pay, worker, employee, veteran, executive_order, healthcare, requirement, profit, section, committee, change, coverage, retirement, fee, expense, field, insurance\n",
      "13: term, impact, solution, debt, assistance, cut, knowledge, commission, hospital, billion, payment, professional, burden, reduction, airport, build, sovereignty, loan_debt, pension, barrier\n",
      "14: life, house, death, police, official, enforcement, officer, polouse, march, russia, killing, son, agent, night, transgender, june, putin, floor, daughter, injury\n",
      "15: power, member, court, medium, mother, candidate, judge, girl, god, limit, attorney, appeal, slaughter, prosecutor, doesn, outrage, faith, claim, office, campaign\n",
      "16: tax, program, event, violence, student, funding, democracy, fund, charge, teacher, increase, privacy, exemption, graduate, person, taxpayer, corruption, witness, payer, notice\n",
      "17: vote, threat, land, city, force, voter, park, aid, china, military, mass, border, shooting, hero, town, isis, iraq, unit, green_card, polling\n",
      "18: today, check, japan, rest, spirit, view, model, magazine, anniversary, residency, summer, challenge, tour, bankruptcy, accusation, workplace, imagination, taiji, indonesia, high_capacity\n",
      "19: year, crime, case, evidence, form, victim, trial, september, dog, assault, blood, jury, trump, abuse, help, voting, perpetrator, jail, woman, type\n"
     ]
    }
   ],
   "source": [
    "ct_model_remove = ct.Corex(n_hidden=(num_topics), words=remove_words, max_iter=5, verbose=False, seed=seed)\n",
    "ct_model_remove.fit(remove_doc_word, words=remove_words, anchors=remove_anchor_words, anchor_strength=6);\n",
    "for n in range(num_topics):\n",
    "    topic_words,_ = zip(*ct_model_remove.get_topics(topic=n, n_words=20))\n",
    "    print '{}: '.format(n) + ', '.join(topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split topic 6\n",
    "toSplitId = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(word1, word2):\n",
    "    return wv_model.wv.similarity(word1, word2)\n",
    " \n",
    "def buildSimilarityMatrix(samples):\n",
    "    numOfSamples = len(samples)\n",
    "    matrix = np.zeros(shape=(numOfSamples, numOfSamples))\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix)):\n",
    "            matrix[i,j] = distance(samples[i], samples[j])\n",
    "    return matrix\n",
    "\n",
    "samples = anchor_words[toSplitId]\n",
    "sim_mat = buildSimilarityMatrix(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'internet', u'consumer', u'energy', u'information', u'technology', u'service', u'device', u'car', u'access', u'safety', u'standard', u'road', u'vehicle', u'com'], [u'provider', u'fuel', u'accident', u'content', u'forest', u'aviation']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "num_cluster = 2 # categorize the words into 2 clusters\n",
    "mat = np.matrix(sim_mat)\n",
    "res = SpectralClustering(num_cluster).fit_predict(mat)\n",
    "ll = [[] for _ in range(num_cluster)]\n",
    "for i in range(len(samples)):\n",
    "    idx = res[i]\n",
    "    word = samples[i]\n",
    "    ll[idx].append(word)\n",
    "\n",
    "print ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new anchor words\n",
    "anchor_words_split = list(anchor_words)\n",
    "del anchor_words_split[toSplitId]\n",
    "for i in range(num_cluster):\n",
    "    anchor_words_split.insert(toSplitId, ll[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: treatment, condition, disease, patient, doctor, cancer, awareness, pain, illness, medicine, disability, disorder, cure, medication, symptom, physician, brain, diagnosis, diabetes, organ\n",
      "1: man_power, consent_government, code_word, father_united, states_declaration, racist_racist, event_person, station_law, god_respect, racist_code, power_earth, band_assume, independence_course, opinion_mankind, nature_nature, separation_government, government_founding, africa_african, immigration_assimilation, genocide_racist\n",
      "2: god_trust, robbery_man, year_september, retrial_notice, state_robbery, wingate_rap, wingate_involvement, christie_president, murder_feel, robbery_kidnapping, icon_max, rapper_conviction, max_appeal, vote_bill, pluribus_unum, house_floor, veteran_exposure, health_registry, deed, veteran_mcclellan\n",
      "3: april, ocean, nasa, explosion, balance, proclaim, bus, shame, abuse_power, cabinet, transit, museum, fiction, specialist, nasa_budget, sentence, year_sentence, crime, dolphin, daughter\n",
      "4: health, benefit, pay, research, company, economy, business, cost, fund, money, tax, industry, dollar, budget, market, study, increase, product, debt, healthcare\n",
      "5: office, investigation, vote, election, campaign, candidate, corruption, voter, hillary_clinton, fraud, voting, attorney, violation, fbi, department_justice, primary, democrat, ballot, doj, bernie_sander\n",
      "6: provider, accident, fuel, content, aviation, forest, species, habitat, conservation, faa, emission, carbon, project, internet_service, wildlife, dwight, acre, time_warner, usfws, engineer\n",
      "7: service, safety, information, com, access, standard, technology, internet, consumer, vehicle, energy, car, device, road, privacy, traffic, http, https_facebook, customer, equipment\n",
      "8: war, freedom, force, power, democracy, leader, party, respect, ukraine, russia, liberty, sanction, border, regime, republic, territory, principle, protester, opposition, freedom_speech\n",
      "9: use, drug, marijuana, animal, sale, owner, plant, substance, cruelty, oil, gas, chemical, cannabis, pet, alcohol, consumption, cigarette, hemp, retrial, farm\n",
      "10: school, community, education, student, science, college, youth, university, teacher, sport, hate, muslim, graduate, goo, skill, assessment, art, educator, contribution, student_loan\n",
      "11: history, land, genocide, word, white, park, race, existence, diversity, hunting, extinction, ecosystem, monument, christmas, archive, trayvon_martin, wolf, celebration, help_veteran, wilderness\n",
      "12: record, japan, racist, clinton, player, fan, music, http_goo, character, william, floor_vote, iii, fighter, korean, reparation, university_student, winner, tomorrow, hillary_rodham, google\n",
      "13: protection, amendment, weapon, gun, firearm, property, polouse, arm, dog, shooting, tragedy, target, gun_violence, police_department, magazine, gun_control, self_defense, meat, michael, film\n",
      "14: help, home, job, work, worker, status, class, week, opportunity, visa, immigration, income, immigrant, program, relief, degree, puerto_rico, wage, compensation, employer\n",
      "15: group, event, attack, obama, terrorism, terrorist, movement, civilian, military, isis, president_barack, iraq, massacre, terrorist_group, terror, zone, syrium, terrorist_organization, eye, treaty\n",
      "16: abuse, solution, water, region, development, travel, employment, application, competition, investment, infrastructure, approach, waste, innovation, uscis, cable, unemployment, ebola, disaster, africa\n",
      "17: evidence, officer, department, police, law_enforcement, killing, judge, county, police_officer, conviction, justice_system, aircraft, arrest, chief, governor_chris, scene, deputy, radiation, murder, jury\n",
      "18: video, ground, head, title, service_member, facebook, defend, courage, installation, crew, white_house, camera, shooter, medal_honor, ship, sen, homeowner, jone, floor, award\n",
      "19: duty, soldier, veteran, speech, army, defense, church, afghanistan, personnel, retirement, troop, initiative, kill, marine, flight, secure, pilot, dea, department_defense, afghan\n",
      "20: support, death, policy, security, organization, trial, barack_obama, difference, bank, jail, transgender, half, negotiation, farmer, intelligence, working, combat, finance, advantage, department_homeland\n"
     ]
    }
   ],
   "source": [
    "# Anchor 'education' and 'weapon' to first topic, 'drug' and 'student' to second topic, so on...\n",
    "# Apparently different words are selected for one topic to demonstrate whether the anchoring works\n",
    "ct_model_split = ct.Corex(n_hidden=(num_topics + num_cluster - 1), words=words, max_iter=5, verbose=False, seed=seed)\n",
    "ct_model_split.fit(doc_word, words=words, anchors=anchor_words_split, anchor_strength=6);\n",
    "for n in range(num_topics + num_cluster - 1):\n",
    "    topic_words,_ = zip(*ct_model_split.get_topics(topic=n, n_words=20))\n",
    "    print '{}: '.format(n) + ', '.join(topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Topics by Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['d', 'e', 'f', 'a', 'b', 'c'], ['g', 'h', 'i'], ['j', 'k', 'l']]\n"
     ]
    }
   ],
   "source": [
    "def merge_topics(idx1, idx2, topic_words):\n",
    "    idx1, idx2 = min(idx1, idx2), max(idx2, idx1)\n",
    "    res = []\n",
    "    for idx, words in enumerate(topic_words):\n",
    "        res.append(list(words))\n",
    "        if(idx == idx2):\n",
    "            res[idx1] = res[idx2] + res[idx1]\n",
    "    del res[idx2]\n",
    "    return res\n",
    "\n",
    "topic_words = [['a','b','c'], ['d','e','f'], ['g','h','i'], ['j','k','l']]\n",
    "res = merge_topics(0, 1, topic_words)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge topic x and topic y\n",
    "anchor_words_merge = merge_topics(5, 16, anchor_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: treatment, condition, disease, patient, doctor, cancer, awareness, pain, illness, medicine, disability, disorder, cure, medication, symptom, physician, brain, diagnosis, diabetes, organ\n",
      "1: man_power, consent_government, code_word, father_united, states_declaration, station_law, event_person, racist_racist, opinion_mankind, god_respect, nature_nature, band_assume, power_earth, separation_government, racist_code, independence_course, government_founding, africa_african, immigration_assimilation, genocide_racist\n",
      "2: god_trust, robbery_man, robbery_kidnapping, murder_feel, retrial_notice, year_september, christie_president, rapper_conviction, wingate_involvement, state_robbery, max_appeal, wingate_rap, icon_max, pluribus_unum, vote_bill, house_floor, health_registry, veteran_exposure, deed, veteran_mcclellan\n",
      "3: april, hunt, ocean, nasa, abuse_power, proclaim, explosion, dolphin, bus, cabinet, zimbabwe, transit, fiction, torture_murder, museum, nasa_budget, news, boston, marathon, york\n",
      "4: health, money, benefit, company, pay, economy, cost, fund, tax, industry, dollar, budget, study, market, increase, product, debt, healthcare, research, business\n",
      "5: investigation, vote, election, officer, police, law_enforcement, campaign, candidate, corruption, voter, judge, hillary_clinton, fraud, voting, attorney, police_officer, department_justice, fbi, primary, ballot\n",
      "6: service, safety, information, access, standard, technology, internet, consumer, vehicle, energy, car, provider, device, accident, fuel, road, content, aviation, forest, com\n",
      "7: power, war, freedom, force, party, democracy, leader, respect, ukraine, russia, liberty, sanction, border, regime, republic, territory, principle, protester, opposition, freedom_speech\n",
      "8: use, drug, marijuana, animal, sale, owner, plant, substance, cruelty, oil, gas, chemical, cannabis, pet, alcohol, ammunition, consumption, cigarette, hemp, retrial\n",
      "9: school, community, education, student, science, college, university, youth, teacher, sport, contribution, hate, graduate, goo, skill, art, assessment, movie, educator, student_loan\n",
      "10: history, land, genocide, word, white, park, race, existence, diversity, hunting, extinction, ecosystem, monument, christmas, prize, archive, trayvon_martin, celebration, wolf, nobel_peace\n",
      "11: record, japan, clinton, player, fan, music, william, character, floor_vote, fighter, university_student, korean, reparation, winner, tomorrow, hillary_rodham, mind_control, game, thinking, comfort_woman\n",
      "12: protection, amendment, weapon, gun, firearm, property, polouse, arm, dog, tragedy, shooting, target, gun_violence, police_department, magazine, gun_control, self_defense, michael, constitution, mass_shooting\n",
      "13: help, home, job, work, program, worker, status, class, week, opportunity, visa, immigration, income, immigrant, relief, degree, puerto_rico, compensation, wage, employer\n",
      "14: group, event, attack, obama, terrorism, terrorist, movement, civilian, military, isis, president_barack, iraq, massacre, terrorist_group, terror, zone, syrium, terrorist_organization, eye, violence\n",
      "15: water, solution, region, development, travel, employment, application, team, disaster, competition, investment, infrastructure, innovation, approach, waste, uscis, cable, unemployment, ebola, abuse\n",
      "16: video, ground, title, head, service_member, facebook, defend, installation, floor, white_house, crew, camera, shooter, ship, medal_honor, sen, jone, homeowner, com_watch, watch\n",
      "17: duty, soldier, veteran, army, defense, religion, church, personnel, afghanistan, retirement, troop, marine, flight, secure, pilot, dea, department_defense, hero, tax_exemption, afghan\n",
      "18: support, policy, security, organization, trial, jail, difference, bank, half, transgender, negotiation, combat, advantage, intelligence, working, farmer, spouse, finance, department_homeland, barack_obama\n"
     ]
    }
   ],
   "source": [
    "ct_model_merge = ct.Corex(n_hidden=(num_topics - 1), words=words, max_iter=20, verbose=False, seed=seed)\n",
    "ct_model_merge.fit(doc_word, words=words, anchors=anchor_words_merge, anchor_strength=6);\n",
    "for n in range(num_topics - 1):\n",
    "    topic_words,_ = zip(*ct_model_merge.get_topics(topic=n, n_words=20))\n",
    "    print '{}: '.format(n) + ', '.join(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
